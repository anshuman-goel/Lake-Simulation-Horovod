################################################################################

Group info:
agoel5 Anshuman Goel
kgondha Kaustubh G Gondhalekar
ndas Neha Das

################################################################################


Problem Objective: 
Implementing multi-node/multi-GPU Tensorflow via MPI through Horovod

Compilation and Execution Instructions:
(Refer to the HW description)

Implementation:
The number of pebbles was divided across the two nodes/grids. Then, we created the send and receive buffers with dimensions 2*N and created tensor variables for them.
Since we have to receive the two top rows of rank 1 in rank 0, we need two extra rows to accomodate these values in the u_init and ut_init arrays. Hence, the dimensions are declared as N+2 * N.
Similarly, the rank 1 dimensions are also declared as N+2 * N.
After the values are calculated from the pebbles, the send buffer of rank 0 is filled with rows N-2 and N-1 while the send buffer of rank 1 is filled with rows 2 and 3 of their respective grids.
Now, we broadcast from the send buffer of rank 0 to rank 1's receive buffer and vice versa. The actual grids are then updated through the receive buffer in the rows N and N+1 for rank 0, and rows 0 and 1 for rank 1.
Using these updated values, the computation iterates all over again.

Results/Discussion:

Compare the execution time of your lake-horo.py against your lake.py using the parameters N=512, npebs=40, num_iter=400. 
Provide possible explanations for the difference in execution times. 


CPU Execution time comparison between lake.py and lake-horo.py

lake.py: ( gtx480 node hence code runs on CPU)

python lake.py 512 40 400
Elapsed time: 7.19218397141 seconds

lake-horo.py: ( gtx480 node hence code runs on CPU)

mpirun -np 2 ./lake-horo.py 512 40 400
Elapsed time: 77.0818769932 seconds
Elapsed time: 77.0905389786 seconds

There is a huge difference between the execution time of lake.py on CPU cores (without MPI), and running lake-horo.py (with MPI).
This accounts mainly because of the communication overhead between the two nodes in the latter case. In every iteration of TensorFlow, the bottom rank will communicate its top rows to the top rank's bottom rows.
The top rank will communicate its bottom rows to the bottom rank's top rows. Since the only major difference between lake.py and lake-horo.py in terms of code is the broadcast function in the latter, this could also be a factor in increasing the execution time.



GPU Execution time comparison between lake.py and lake-horo.py ( gtx780 node)

python lake.py 512 40 400
Elapsed time: 1.66718006134 seconds

mpirun -np 2 ./lake-horo.py 512 40 400
Elapsed time: 5.13121509552 seconds
Elapsed time: 5.12291288376 seconds

The elapsed times are somewhat comparable here. The extra time taken by lake-horo.py is the communication overhead between the 2 MPI nodes. In every iteration of Tensorflow, the bottom rank will communicate its top rows to the top rank's bottom rows. The top rank will communicate its bottom rows to the bottom rank's top rows.   



Lessons learnt:
We understood how Horovod uses MPI and TensorFlow to perform communication and computations across multiple nodes/GPUs in a cluster. 
We also learnt how to write TensorFlow programs, and also how to create graphs in TensorFlow and its execution model.



----------------EXTRA CREDIT--------------------


Comparison between lake-horo.py (using capability 3.0 GPUs) with GPU+MPI version of HW2 P3:
We modified the lake files by changing the grid size to match the lake-horo.py grid structure and getting a one-to-one comparison.  

HW2 P3 modifications:


 	<-- n -->
|	|---------|---------|
|	|	Q1	  |			|
	|		  |			|
2n	|---------|---------|
	|	Q2	  |			|
| 	|		  |			|
|	|---------|---------|



We are only using quadrants Q1 and Q2 for a one-to-one mapping to lake-horo.py
We have disabled all the column communication from the original code and are only sending the top and bottom rows of quadrants Q2 and Q1 to each other respectively.
We have also restricted the pebble generation and the ripple computation to only quadrants Q1 and Q2. This means our program only requires ( and enforces ) 2 Nodes to be run.

srun -pgtx780 -N2 -n2 --pty /bin/bash
make
prun ./lake 512 40 400.0 1

GPU computation: 2795.642822 msec
GPU took 3.752016 seconds


The modified HW2 (V3) takes 3.75 seconds while the lake-horo.py code on gtx780 node takes, about 5 seconds.

mpirun -np 2 ./lake-horo.py 512 40 400

Elapsed time: 5.13121509552 seconds
Elapsed time: 5.12291288376 seconds


The lake-horo.py has a Tensorflow overhead which involves generation of the data-flow graph and optimizations which probably causes it to take more time. TensorFlow implementation will probably win in performance when there are large computations performed with a heavy scope of optimization which will be difficult to implement manually.

------------------------------------------------
