################################################################################

Group info:
agoel5 Anshuman Goel
kgondha Kaustubh G Gondhalekar
ndas Neha Das

################################################################################


Problem Objective: 
Implementing multi-node/multi-GPU Tensorflow via MPI through Horovod

Compilation and Execution Instructions:
(Refer to the HW description)

Implementation:
The number of pebbles was divided across the two nodes/grids. Then, we created the send and receive buffers with dimensions 2*N and created tensor variables for them.
Since we have to receive the two top rows of rank 1 in rank 0, we need two extra rows to accomodate these values in the u_init and ut_init arrays. Hence, the dimensions are declared as N+2 * N.
Similarly, the rank 1 dimensions are also declared as N+2 * N.
After the values are calculated from the pebbles, the send buffer of rank 0 is filled with rows N-2 and N-1 while the send buffer of rank 1 is filled with rows 2 and 3 of their respective grids.
Now, we broadcast from the send buffer of rank 0 to rank 1's receive buffer and vice versa. The actual grids are then updated through the receive buffer in the rows N and N+1 for rank 0, and rows 0 and 1 for rank 1.
Using these updated values, the computation iterates all over again.

Results/Discussion:

Compare the execution time of your lake-horo.py against your lake.py using the parameters N=512, npebs=40, num_iter=400. 
Provide possible explanations for the difference in execution times. 


CPU Execution time comparison between lake.py and lake-horo.py

lake.py: ( gtx480 node hence code runs on CPU)

python lake.py 512 40 400
Elapsed time: 7.19218397141 seconds

lake-horo.py: ( gtx480 node hence code runs on CPU)

mpirun -np 2 ./lake-horo.py 512 40 400
Elapsed time: 77.0818769932 seconds
Elapsed time: 77.0905389786 seconds

There is a huge difference between the execution time of lake.py on CPU cores (without MPI), and running lake-horo.py (with MPI).
This accounts mainly because of the communication overhead between the two nodes in the latter case. In every iteration of tensorflow, the bottom rank will communicate its top rows to the top rank's bottom rows.
The top rank will communicate its bottom rows to the bottom rank's top rows.   



GPU Execution time comparison between lake.py and lake-horo.py ( gtx780 node)

python lake.py 512 40 400
Elapsed time: 1.66718006134 seconds

mpirun -np 2 ./lake-horo.py 512 40 400
Elapsed time: 5.13121509552 seconds
Elapsed time: 5.12291288376 seconds

The elapsed times are somewhat comparable here. The extra time taken by lake-horo.py is the communication overhead between the 2 MPI nodes. In every iteration of tensorflow, the bottom rank will communicate its top rows to the top rank's bottom rows. The top rank will communicate its bottom rows to the bottom rank's top rows.   



Lessons learnt:
We understood how Horovod uses MPI and TensorFlow to perform communication and computations across multiple nodes/GPUs in a cluster. 
we also learnt how to write TensorFlow programs, and also how to create graphs in TensorFlow and its execution model.



----------------EXTRA CREDIT--------------------



------------------------------------------------
